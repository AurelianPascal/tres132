import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{StructField, StructType, StringType}
import org.apache.spark.sql.functions.{col, lit}
import org.scalatest.FunSuite
import org.scalatest.BeforeAndAfter

class DataFrameAlignmentTest extends FunSuite with BeforeAndAfter {

  var spark: SparkSession = _

  before {
    // Initialize SparkSession
    spark = SparkSession.builder()
      .master("local")
      .appName("DataFrame Alignment Test")
      .getOrCreate()
  }

  after {
    if (spark != null) {
      spark.stop()
    }
  }

  test("Column Renaming and Addition") {
    // Sample DataFrame
    val df = spark.createDataFrame(Seq(
      (1, "Alice"),
      (2, "Bob")
    )).toDF("id", "name")

    // Column mappings: Original Column Name -> New Column Name
    val columnMappings = Map("id" -> "customer_id", "name" -> "customer_name", "age" -> "customer_age")

    // Expected Schema
    val expectedSchema = StructType(List(
      StructField("customer_id", StringType, true),
      StructField("customer_name", StringType, true),
      StructField("customer_age", StringType, true)
    ))

    // Align DataFrame
    val alignedDf = alignDataFrameWithMappings(spark, df, columnMappings)

    // Test Column Renaming and Addition
    assert(alignedDf.schema === expectedSchema)

    // Test Data Integrity
    val rows = alignedDf.collect()
    assert(rows(0).getAs[String]("customer_name") === "Alice")
    assert(rows(1).getAs[String]("customer_name") === "Bob")

    // Test Null Values for Added Columns
    assert(rows.forall(_.isNullAt(alignedDf.schema.fieldIndex("customer_age"))))
  }

  // More tests can be added here to cover edge cases, such as empty mappings, DataFrame, or duplicate column names.
}


###############

test("filterColumnsStartsWith - include true") {
    val df = spark.createDataFrame(Seq(
      (1, "Alice", 100),
      (2, "Bob", 200)
    )).toDF("id", "name", "salary")

    val result = filterColumnsStartsWith(df, Seq("name", "sal"), include = true)
    assert(result.columns.sameElements(Array("name", "salary")))
  }

  test("filterColumnsStartsWith - include false") {
    val df = spark.createDataFrame(Seq(
      (1, "Alice", 100),
      (2, "Bob", 200)
    )).toDF("id", "name", "salary")

    val result = filterColumnsStartsWith(df, Seq("name", "sal"), include = false)
    assert(result.columns.sameElements(Array("id")))
  }

  test("alignDataFramesSchemas") {
    val df1 = spark.createDataFrame(Seq((1, "Alice"))).toDF("Id", "Name")
    val df2 = spark.createDataFrame(Seq((100, "Engineering"))).toDF("salary", "department")

    val result = alignDataFramesSchemas(Seq(df1, df2))
    assert(result.forall(_.columns.length == 4))
    assert(result.head.columns.contains("id"))
    assert(result.head.columns.contains("name"))
    assert(result.head.columns.contains("salary"))
    assert(result.head.columns.contains("department"))
  }

  test("toLowerCaseDF") {
    val df = spark.createDataFrame(Seq((1, "Alice", 100))).toDF("ID", "Name", "Salary")
    val result = toLowerCaseDF(df)

    assert(result.columns.sameElements(Array("id", "name", "salary")))
    assert(result.collect()(0).getAs[String]("name") == "Alice")
  }


##

import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}
import org.apache.spark.sql.Row

test("loadDataFrame should load a DataFrame and convert column names to lowercase") {
  // Step 1: Create a DataFrame with uppercase column names
  val schema = StructType(
    List(
      StructField("ID", IntegerType, nullable = false),
      StructField("NAME", StringType, nullable = false)
    )
  )
  
  val data = spark.sparkContext.parallelize(Seq(Row(1, "Alice"), Row(2, "Bob")))
  val dfWithUpperCaseColumns = spark.createDataFrame(data, schema)
  
  // Step 2: Create a temporary view from this DataFrame
  dfWithUpperCaseColumns.createOrReplaceTempView("test_table")
  
  // Step 3: Invoke loadDataFrame on this "table"
  val loadedDf = ReconciliationUtility.loadDataFrame(spark, "test_table")
  
  // Step 4: Assert that all column names in the returned DataFrame are lowercase
  assert(loadedDf.columns.forall(_.forall(_.isLower)), "Not all columns are lowercase")
}


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import org.scalatest.flatspec.AnyFlatSpec
import org.scalatest.matchers.should.Matchers
import org.mockito.Mockito._
import org.mockito.ArgumentMatchers._

class FindAndWriteBreaksTest extends AnyFlatSpec with Matchers {

  "findAndWriteBreaks" should "find breaks and write to SQL Server" in {
    // Mock SparkSession and DataFrame
    val mockSpark = mock(classOf[SparkSession])
    val mockSourceDf = mock(classOf[DataFrame])
    val mockBreaksDf = mock(classOf[DataFrame])

    // Stub the source DataFrame to simulate the creation of a temp view
    doNothing().when(mockSourceDf).createOrReplaceTempView("source_view")

    // Stub SparkSession.sql to return the mockBreaksDf DataFrame
    when(mockSpark.sql(anyString())).thenReturn(mockBreaksDf)

    // Stub for writeToSqlServer (assuming you can access it for mocking)
    // You may need to adjust this part depending on how you can intercept or mock this call
    doNothing().whenStatic(classOf[YourTestClass]).writeToSqlServer(any[DataFrame], any[Map[String, String]])

    // Prepare parameters for the function
    val selectColumns = Array("uuid", "event_version", "event_lastupdated")
    val comparisonTableName = "comparisonTable"
    val serverName = "server"
    val databaseName = "database"
    val breaksTableName = "breaksTable"
    val user = "user"
    val password = "password"

    // Call the function under test
    YourTestClass.findAndWriteBreaks(mockSpark, mockSourceDf, selectColumns, comparisonTableName, serverName, databaseName, breaksTableName, user, password)

    // Verify that the correct SQL query was executed
    verify(mockSpark).sql(contains("FROM source_view s LEFT JOIN comparisonTable c"))

    // Verify that the breaks DataFrame was attempted to be written to SQL Server with correct parameters
    // Again, adjust verification based on your mocking strategy for writeToSqlServer
    verifyStatic(classOf[YourTestClass])
    YourTestClass.writeToSqlServer(eq(mockBreaksDf), any[Map[String, String]])
  }
}

#####

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.scalatest.flatspec.AnyFlatSpec
import org.scalatest.matchers.should.Matchers
import org.scalatestplus.mockito.MockitoSugar
import org.mockito.Mockito._
import org.mockito.ArgumentMatchers._

class YourTestObjectTest extends AnyFlatSpec with Matchers with MockitoSugar {

  "YourTestObject" should "find breaks and write to SQL Server" in {
    val mockSparkSession = mock[SparkSession]
    val mockSourceDf = mock[DataFrame]
    val mockBreaksDf = mock[DataFrame]
    val sqlWriterMock = mock[SqlServerWriter]

    // Assuming YourTestObject can somehow use a mocked SqlServerWriter (not shown here)
    val yourTestObjectUnderTest = YourTestObject
    yourTestObjectUnderTest.sqlWriter = sqlWriterMock // This line is pseudo-code

    when(mockSparkSession.sql(any[String])).thenReturn(mockBreaksDf)
    doNothing().when(mockSourceDf).createOrReplaceTempView(anyString())

    yourTestObjectUnderTest.findAndWriteBreaks(
      mockSparkSession, mockSourceDf, "comparisonTable", "serverName", "databaseName",
      "breaksTableName", "user", "password"
    )

    verify(mockSparkSession).sql(contains("FROM source_view s LEFT JOIN comparisonTable c"))
    verify(sqlWriterMock).writeToSqlServer(eq(mockBreaksDf), any[Map[String, String]])
  }
}




