import logging
from pyspark.sql import SparkSession, functions as F
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Start time
start_time = time.time()
logging.info("Script started.")

# SQL Server connection details
jdbc_url = "jdbc:sqlserver://[ServerName];databaseName=[DatabaseName]"
jdbc_properties = {
    "user": "[Username]",
    "password": "[Password]",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Initialize Spark Session
logging.info("Initializing Spark Session.")
spark = SparkSession.builder.appName("DeltaLake_BondEquity_Union_Filtered").getOrCreate()

# Function to convert column names to lowercase, filter by max AsOfDate, drop year and month columns, and keep only string columns
def process_dataframe(df):
    df = df.select([F.col(c).alias(c.lower()) for c in df.columns])
    df = df.withColumn("asofdate", F.to_date(F.col("asofdate")))

    # Find the maximum asofdate
    max_asofdate = df.agg(F.max("asofdate").alias("max_asofdate")).collect()[0]["max_asofdate"]

    # Filter by the maximum asofdate
    df = df.filter(F.col("asofdate") == max_asofdate)

    # Select only string columns
    string_columns = [col_name for col_name, dtype in df.dtypes if dtype == 'string']
    df = df.select(*string_columns)

    return df.limit(1000)

# Load the tables, process, and take only 1000 rows from each
logging.info("Loading, processing, and limiting bond.issue and equity.issue tables.")
bond_issue_df = process_dataframe(spark.table("bond.issue"))
equity_issue_df = process_dataframe(spark.table("equity.issue"))

# Exclude columns starting with 'tl'
logging.info("Excluding columns starting with 'tl'.")
bond_issue_df = bond_issue_df.select(*[c for c in bond_issue_df.columns if not c.startswith('tl')])
equity_issue_df = equity_issue_df.select(*[c for c in equity_issue_df.columns if not c.startswith('tl')])

# Add any additional processing or actions here...

# End time
end_time = time.time()
logging.info(f"Script ended. Total execution time: {end_time - start_time} seconds.")

###########

import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import StringType
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Start time
start_time = time.time()
logging.info("Script started.")

# SQL Server connection details
jdbc_url = "jdbc:sqlserver://[ServerName];databaseName=[DatabaseName]"
jdbc_properties = {
    "user": "[Username]",
    "password": "[Password]",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Initialize Spark Session
logging.info("Initializing Spark Session.")
spark = SparkSession.builder.appName("DeltaLake_BondEquity_Union_Filtered").getOrCreate()

# Function to convert column names to lowercase, filter by max AsOfDate, drop year and month columns, and cast void columns to string
def process_dataframe(df):
    df = df.select([F.col(c).alias(c.lower()) for c in df.columns])
    df = df.withColumn("asofdate", F.to_date(F.col("asofdate")))

    # Find the maximum asofdate
    max_asofdate = df.agg(F.max("asofdate").alias("max_asofdate")).collect()[0]["max_asofdate"]

    # Filter by the maximum asofdate
    df = df.filter(F.col("asofdate") == max_asofdate)

    # Cast VoidType columns to StringType
    for column, dtype in df.dtypes:
        if dtype == 'void':
            df = df.withColumn(column, F.col(column).cast(StringType()))

    return df.limit(1000)

# Load the tables, process, and take only 1000 rows from each
logging.info("Loading, processing, and limiting bond.issue and equity.issue tables.")
bond_issue_df = process_dataframe(spark.table("bond.issue"))
equity_issue_df = process_dataframe(spark.table("equity.issue"))

# Exclude columns starting with 'tl'
logging.info("Excluding columns starting with 'tl'.")
bond_issue_df = bond_issue_df.select(*[c for c in bond_issue_df.columns if not c.startswith('tl')])
equity_issue_df = equity_issue_df.select(*[c for c in equity_issue_df.columns if not c.startswith('tl')])

# Add any additional processing or actions here...

# End time
end_time = time.time()
logging.info(f"Script ended. Total execution time: {end_time - start_time} seconds.")


########

import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import StringType
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Start time
start_time = time.time()
logging.info("Script started.")

# SQL Server connection details
jdbc_url = "jdbc:sqlserver://[ServerName];databaseName=[DatabaseName]"
jdbc_properties = {
    "user": "[Username]",
    "password": "[Password]",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Initialize Spark Session
logging.info("Initializing Spark Session.")
spark = SparkSession.builder.appName("DeltaLake_FinancialData_Processing").getOrCreate()

# Function to process DataFrame and convert VoidType columns to StringType
def process_dataframe(df):
    # Convert VoidType columns to StringType
    for column, dtype in df.dtypes:
        if dtype == 'void':
            df = df.withColumn(column, F.lit(None).cast(StringType()))

    # Your existing processing code here...

    return df

# Load and process the tables
logging.info("Loading and processing financial tables.")
bond_issue_df = process_dataframe(spark.table("bond.issue"))
equity_issue_df = process_dataframe(spark.table("equity.issue"))
options_df = process_dataframe(spark.table("options.issue"))
futures_df = process_dataframe(spark.table("futures.issue"))

# Get the union of all column names
all_columns = set(bond_issue_df.columns) | set(equity_issue_df.columns) | set(options_df.columns) | set(futures_df.columns)

# Function to add missing columns to a DataFrame
def add_missing_columns(df, columns):
    missing_columns = columns - set(df.columns)
    for col in missing_columns:
        df = df.withColumn(col, F.lit(None).cast(StringType()))
    return df

# Add missing columns to each DataFrame and select columns in consistent order
bond_issue_df = add_missing_columns(bond_issue_df, all_columns).select(*all_columns)
equity_issue_df = add_missing_columns(equity_issue_df, all_columns).select(*all_columns)
options_df = add_missing_columns(options_df, all_columns).select(*all_columns)
futures_df = add_missing_columns(futures_df, all_columns).select(*all_columns)

# Add any additional processing or actions here...

# End time
end_time = time.time()
logging.info(f"Script ended. Total execution time: {end_time - start_time} seconds.")


#####

import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import StringType
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format=''%(asctime)s - %(levelname)s - %(message)s'')

# Start time
start_time = time.time()
logging.info("Script started.")

# SQL Server connection details
jdbc_url = "jdbc:sqlserver://[ServerName];databaseName=[DatabaseName]"
jdbc_properties = {
    "user": "[Username]",
    "password": "[Password]",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Initialize Spark Session
logging.info("Initializing Spark Session.")
spark = SparkSession.builder.appName("DeltaLake_FinancialData_Processing").getOrCreate()

# Function to process DataFrame and convert VoidType columns to StringType
def process_dataframe(df):
    # Convert VoidType columns to StringType
    for column, dtype in df.dtypes:
        if dtype == 'void':
            df = df.withColumn(column, F.lit(None).cast(StringType()))

    # Your existing processing code here...

    return df

# Load and process the tables
logging.info("Loading and processing financial tables.")
bond_issue_df = process_dataframe(spark.table("bond.issue"))
equity_issue_df = process_dataframe(spark.table("equity.issue"))
options_df = process_dataframe(spark.table("options.issue"))
futures_df = process_dataframe(spark.table("futures.issue"))

# Get the union of all column names and sort them
all_columns = sorted(set(bond_issue_df.columns) | set(equity_issue_df.columns) | set(options_df.columns) | set(futures_df.columns))

# Function to add missing columns to a DataFrame
def add_missing_columns(df, columns):
    missing_columns = [col for col in columns if col not in df.columns]
    for col in missing_columns:
        df = df.withColumn(col, F.lit(None).cast(StringType()))
    return df

# Add missing columns to each DataFrame and select columns in consistent order
bond_issue_df = add_missing_columns(bond_issue_df, all_columns).select(*all_columns)
equity_issue_df = add_missing_columns(equity_issue_df, all_columns).select(*all_columns)
options_df = add_missing_columns(options_df, all_columns).select(*all_columns)
futures_df = add_missing_columns(futures_df, all_columns).select(*all_columns)

# Add any additional processing or actions here...

# End time
end_time = time.time()
logging.info(f"Script ended. Total execution time: {end_time - start_time} seconds.")


]



# Union all DataFrames
all_dataframes_combined = bond_issue_df.union(equity_issue_df)\
                                       .union(options_df)\
                                       .union(futures_df)

# Repartition the DataFrame for optimized writing
partitions = 10
all_dataframes_combined = all_dataframes_combined.repartition(partitions)


############################################################
'###############

import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import StringType
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Start time
start_time = time.time()
logging.info("Script started.")

# SQL Server connection details
jdbc_url = "jdbc:sqlserver://[ServerName];databaseName=[DatabaseName]"
jdbc_properties = {
    "user": "[Username]",
    "password": "[Password]",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Initialize Spark Session
logging.info("Initializing Spark Session.")
spark = SparkSession.builder.appName("DeltaLake_FinancialData_Processing").getOrCreate()

# Function to process DataFrame and convert VoidType columns to StringType, and filter by third latest asofdate
def process_dataframe(df):
    df = df.select([F.col(c).alias(c.lower()) for c in df.columns])
    df = df.withColumn("asofdate", F.to_date(F.col("asofdate")))

    # Find the third latest asofdate
    third_latest_date = df.select("asofdate")\
                          .distinct()\
                          .orderBy(F.desc("asofdate"))\
                          .limit(3)\
                          .collect()[2]["asofdate"]

    # Filter by the third latest asofdate and onwards
    df = df.filter(F.col("asofdate") >= third_latest_date)

    # Convert VoidType columns to StringType
    for column, dtype in df.dtypes:
        if dtype == 'void':
            df = df.withColumn(column, F.lit(None).cast(StringType()))

    return df.limit(1000)

# Load and process the tables
logging.info("Loading and processing financial tables.")
bond_issue_df = process_dataframe(spark.table("bond.issue"))
equity_issue_df = process_dataframe(spark.table("equity.issue"))
options_df = process_dataframe(spark.table("options.issue"))
futures_df = process_dataframe(spark.table("futures.issue"))

# Union all DataFrames
all_dataframes_combined = bond_issue_df.union(equity_issue_df)\
                                       .union(options_df)\
                                       .union(futures_df)

# Repartition the DataFrame for optimized writing
partitions = 10
all_dataframes_combined = all_dataframes_combined.repartition(partitions)

# Write the combined DataFrame to SQL Server
logging.info("Writing the combined DataFrame to SQL Server.")
all_dataframes_combined.write \
    .format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", "all_dataframes") \
    .option("user", jdbc_properties["user"]) \
    .option("password", jdbc_properties["password"]) \
    .option("driver", jdbc_properties["driver"]) \
    .mode("append")  # use "overwrite" if you want to replace the table
    .save()

logging.info("Data written to SQL Server.")

# End time
end_time = time.time()
logging.info(f"Script ended. Total execution time: {end_time - start_time} seconds.")


###################

from pyspark.sql import DataFrame

def filter_columns_startswith(df: DataFrame, columns_prefixes: list, include=True) -> DataFrame:
    """
    Include or exclude columns from a DataFrame based on whether they start with any string in columns_prefixes.

    :param df: The input DataFrame.
    :param columns_prefixes: A list of string prefixes to match column names against.
    :param include: Flag to determine whether to include or exclude the columns. Default is True.
    :return: Modified DataFrame.
    """
    if include:
        # Include columns that start with any of the prefixes in columns_prefixes
        return df.select(*[col for col in df.columns if any(col.startswith(prefix) for prefix in columns_prefixes)])
    else:
        # Exclude columns that start with any of the prefixes in columns_prefixes
        return df.select(*[col for col in df.columns if not any(col.startswith(prefix) for prefix in columns_prefixes)])

# Example usage:
# Assuming 'spark_df' is your Spark DataFrame
columns_to_filter = ["col_prefix1", "col_prefix2"]
modified_df = filter_columns_startswith(spark_df, columns_to_filter, include=True)  # Set include=False to exclude




