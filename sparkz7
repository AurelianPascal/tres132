import logging
from pyspark.sql import SparkSession, functions as F
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Start time
start_time = time.time()
logging.info("Script started.")

# SQL Server connection details
jdbc_url = "jdbc:sqlserver://[ServerName];databaseName=[DatabaseName]"
jdbc_properties = {
    "user": "[Username]",
    "password": "[Password]",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Initialize Spark Session
logging.info("Initializing Spark Session.")
spark = SparkSession.builder.appName("DeltaLake_BondEquity_Union_Filtered").getOrCreate()

# Function to convert column names to lowercase, filter by max AsOfDate, and drop year and month columns
def process_dataframe(df):
    df = df.select([F.col(c).alias(c.lower()) for c in df.columns])
    df = df.withColumn("asofdate", F.to_date(F.col("asofdate")))

    # Find the maximum asofdate
    max_asofdate = df.agg(F.max("asofdate").alias("max_asofdate")).collect()[0]["max_asofdate"]

    # Filter by the maximum asofdate
    df = df.filter(F.col("asofdate") == max_asofdate)

    df = df.drop('year', 'month')
    return df.limit(1000)

# Load the tables, process, and take only 1000 rows from each
logging.info("Loading, processing, and limiting bond.issue and equity.issue tables.")
bond_issue_df = process_dataframe(spark.table("bond.issue"))
equity_issue_df = process_dataframe(spark.table("equity.issue"))

# Exclude columns starting with 'tl'
logging.info("Excluding columns starting with 'tl'.")
bond_issue_df = bond_issue_df.select(*[c for c in bond_issue_df.columns if not c.startswith('tl')])
equity_issue_df = equity_issue_df.select(*[c for c in equity_issue_df.columns if not c.startswith('tl')])

# Add any additional processing or actions here...

# End time
end_time = time.time()
logging.info(f"Script ended. Total execution time: {end_time - start_time} seconds.")
