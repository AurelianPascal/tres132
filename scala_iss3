import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions.{col, lit}
import org.apache.spark.sql.types.StructType

def alignDataFrameToSQLTable(spark: SparkSession, sqlTableName: String, df: DataFrame, jdbcUrl: String, connectionProperties: Properties): DataFrame = {
  // Infer schema from SQL Server table by reading zero rows
  val dbTable = s"(SELECT TOP 0 * FROM $sqlTableName) as subquery"
  val sqlTableSchemaDf = spark.read.jdbc(jdbcUrl, dbTable, connectionProperties)
  val sqlTableSchema: StructType = sqlTableSchemaDf.schema

  // Align DataFrame to SQL table schema
  val dfColumns = df.columns.toSet
  val sqlColumns = sqlTableSchema.fields.map(_.name).toSet

  // Columns to add to DataFrame (exist in SQL table but not in DataFrame)
  val columnsToAdd = sqlColumns.diff(dfColumns).map(c => lit(null).cast(sqlTableSchema(c).dataType).alias(c))

  // Columns to remove from DataFrame (exist in DataFrame but not in SQL table)
  val columnsToSelect = dfColumns.intersect(sqlColumns).map(c => col(c))

  // Apply transformations
  val alignedDf = df.select(columnsToSelect.map(col): _*).selectExpr("*", columnsToAdd.toSeq: _*)

  alignedDf
}

// Usage example
val spark = SparkSession.builder.appName("DataFrameSQLAlignment").getOrCreate()
val jdbcUrl = "jdbc:sqlserver://<your_server>:<port>;databaseName=<your_database>"
val connectionProperties = new Properties()
connectionProperties.setProperty("user", "<your_username>")
connectionProperties.setProperty("password", "<your_password>")
connectionProperties.setProperty("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")

// Assume `df` is your DataFrame
val alignedDf = alignDataFrameToSQLTable(spark, "yourSQLTableName", df, jdbcUrl, connectionProperties)

// Show result
alignedDf.show()



import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import java.util.Properties
import org.junit.jupiter.api.Test
import org.junit.jupiter.api.Assertions._
import org.mockito.Mockito._
import org.mockito.ArgumentMatchers._

class DataFrameSQLAlignmentTest {

  @Test
  def testAlignDataFrameToSQLTable(): Unit = {
    // Initialize SparkSession with Mockito mock
    val spark = mock(classOf[SparkSession])
    val df = mock(classOf[DataFrame])
    val alignedDf = mock(classOf[DataFrame])

    // Mocking the read operation to return a DataFrame with a specific schema
    val jdbcUrl = "jdbc:sqlserver://<your_server>:<port>;databaseName=<your_database>"
    val connectionProperties = new Properties()
    connectionProperties.setProperty("user", "<your_username>")
    connectionProperties.setProperty("password", "<your_password>")
    connectionProperties.setProperty("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")

    // Assuming sqlTableSchemaDf represents the DataFrame that simulates the SQL table schema
    val sqlTableSchemaDf = mock(classOf[DataFrame])

    when(spark.read.jdbc(eqTo(jdbcUrl), anyString(), any(classOf[Properties]))).thenReturn(sqlTableSchemaDf)
    
    // Assume these are the schema adjustments your function would make
    when(df.columns).thenReturn(Array("existingColumn"))
    when(sqlTableSchemaDf.schema).thenReturn(StructType(Array(StructField("existingColumn", StringType, true), StructField("newColumn", StringType, true))))
    when(df.select("existingColumn")).thenReturn(df) // Mock select to return some DataFrame, for simplicity
    when(df.selectExpr("*", "null as newColumn")).thenReturn(alignedDf) // Mock transformation

    // Actual function call
    val result = alignDataFrameToSQLTable(spark, "yourSQLTableName", df, jdbcUrl, connectionProperties)

    // Verify the result
    assertEquals(alignedDf, result)
    
    // Verify interactions - as per your logic
    verify(df).select("existingColumn") // Verify that select was called with correct columns
    verify(df).selectExpr("*", "null as newColumn") // Verify that transformation was applied correctly
  }
}


//////////

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions.{col, lit}
import org.apache.spark.sql.types.StructType

def alignDataFrameToSQLTable(spark: SparkSession, sqlTableName: String, df: DataFrame, jdbcUrl: String, readOptions: Map[String, String]): DataFrame = {
  // Define the SQL query for schema inference (selecting zero rows)
  val query = s"(SELECT TOP 0 * FROM $sqlTableName) as subquery"
  val options = readOptions + ("url" -> jdbcUrl) + ("dbtable" -> query)
  
  // Use Spark's DataFrameReader with format "jdbc" to infer schema
  val sqlTableSchemaDf = spark.read.format("jdbc").options(options).load()
  val sqlTableSchema: StructType = sqlTableSchemaDf.schema

  // Align DataFrame to SQL table schema
  val dfColumns = df.columns.toSet
  val sqlColumns = sqlTableSchema.fields.map(_.name).toSet

  // Columns to add to DataFrame (exist in SQL table but not in DataFrame)
  val columnsToAdd = sqlColumns.diff(dfColumns).map(c => 
    lit(null).cast(sqlTableSchema(c).dataType).alias(c)
  )

  // Columns to remove from DataFrame (exist in DataFrame but not in SQL table)
  val columnsToSelect = dfColumns.intersect(sqlColumns).map(c => col(c))

  // Apply transformations
  val alignedDf = df.select(columnsToSelect.map(col): _*).selectExpr("*", columnsToAdd.toSeq: _*)

  alignedDf
}

// Usage example
val spark = SparkSession.builder.appName("DataFrameSQLAlignment").getOrCreate()
val jdbcUrl = "jdbc:sqlserver://<your_server>:<port>;databaseName=<your_database>"
val readOptions = Map(
  "user" -> "<your_username>",
  "password" -> "<your_password>",
  "driver" -> "com.microsoft.sqlserver.jdbc.SQLServerDriver"
)

// Assume `df` is your DataFrame
val alignedDf = alignDataFrameToSQLTable(spark, "yourSQLTableName", df, jdbcUrl, readOptions)

// Show result
alignedDf.show()


////
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import java.util.Properties

/**
 * Loads column mappings from a SQL table named "my_mapping_table" using JDBC.
 *
 * @param spark The Spark session.
 * @param jdbcUrl The JDBC URL for the database connection.
 * @param properties JDBC connection properties including user and password.
 * @return A Map where keys are Delta column names (map_from_delta) and values are SQL column names (map_to_sql).
 */
def loadColumnMappingsFromSQL(spark: SparkSession, jdbcUrl: String, properties: Properties): Map[String, String] = {
  // Define the query to select only the rows with datatype='ISSUE'
  val query = "(SELECT map_from_delta, map_to_sql FROM my_mapping_table WHERE datatype = 'ISSUE') AS issue_mappings"

  // Read the mapping table using JDBC
  val mappingDf = spark.read
    .jdbc(jdbcUrl, query, properties)

  // Collect the mappings into a Map
  val mappings = mappingDf.collect()
    .map(row => row.getAs[String]("map_from_delta") -> row.getAs[String]("map_to_sql"))
    .toMap

  mappings
}

// Usage example
val spark = SparkSession.builder.appName("DataFrameSchemaAlignment").getOrCreate()
val jdbcUrl = "jdbc:sqlserver://<your_server>:<port>;databaseName=<your_database>"
val properties = new Properties()
properties.setProperty("user", "<your_username>")
properties.setProperty("password", "<your_password>")
properties.setProperty("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")

// Load column mappings from the SQL table "my_mapping_table"
val columnMappings = loadColumnMappingsFromSQL(spark, jdbcUrl, properties)

// Assuming `df` is your DataFrame representing the Delta table, and you have defined `alignDataFrameWithMappings` as before
// val alignedDf = alignDataFrameWithMappings(spark, df, columnMappings)

// Show result
// alignedDf.show()


/**
 * Aligns a DataFrame to match a specified schema, using column mappings.
 * 
 * @param spark The Spark session.
 * @param df The DataFrame to align.
 * @param columnMappings A map where keys are Delta table column names and values are SQL table column names.
 * @return A DataFrame aligned to the schema defined by columnMappings, with columns added or dropped as necessary.
 */
def alignDataFrameWithMappings(spark: SparkSession, df: DataFrame, columnMappings: Map[String, String]): DataFrame = {
  // Generate the SQL table schema based on the mappings provided
  val sqlTableSchema = StructType(columnMappings.values.toList.distinct.map(StructField(_, StringType, true)))

  // Align DataFrame to the schema defined by column mappings
  // Add missing columns (present in SQL schema but not in DataFrame)
  val columnsToAdd = sqlTableSchema.fields
    .filterNot(field => df.columns.contains(field.name) || columnMappings.values.toList.contains(field.name))
    .map(field => lit(null).cast(StringType).alias(field.name))

  // Select and rename columns as necessary to match the SQL table column names
  val columnsToSelect = df.columns
    .flatMap(deltaName => columnMappings.get(deltaName).orElse(Some(deltaName)).map(sqlName => col(deltaName).alias(sqlName)))

  // Apply transformations
  val alignedDf = df.select(columnsToSelect ++ columnsToAdd: _*)

  alignedDf
}




