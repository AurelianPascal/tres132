import org.apache.spark.sql.{Row, SparkSession, DataFrame}
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import org.mockito.ArgumentMatchers.any
import org.mockito.Mockito.{doNothing, mock, when}
import org.scalatest.{BeforeAndAfterAll, Suite}
import org.scalatest.funsuite.AnyFunSuite
import org.scalatest.matchers.should.Matchers
import org.scalatestplus.mockito.MockitoSugar

class ReconciliationUtilityTest extends AnyFunSuite with MockitoSugar with Matchers with BeforeAndAfterAll with Suite {

  @transient var spark: SparkSession = _

  override def beforeAll(): Unit = {
    super.beforeAll()
    spark = SparkSession.builder()
      .master("local[2]")
      .appName("ReconciliationUtilityTest")
      .getOrCreate()
  }

  override def afterAll(): Unit = {
    if (spark != null) {
      spark.stop()
    }
    super.afterAll()
  }

  test("loadDataFrame should load a DataFrame and lowercase column names") {
    val reconciliationUtility = new ReconciliationUtility()

    val schema = StructType(List(
      StructField("ID", StringType, nullable = true),
      StructField("Date", StringType, nullable = true)
    ))
    val data = Seq(Row("1", "2020-01-01"), Row("2", "2020-01-02"))
    val rdd = spark.sparkContext.parallelize(data)
    val df = spark.createDataFrame(rdd, schema)

    val loadedDf = reconciliationUtility.loadDataFrame(spark, "dummyTable") // Assuming dummyTable exists or mocking this operation

    // Verify the columns are lowercase
    loadedDf.columns should contain theSameElementsAs Seq("id", "date")
  }

  test("filterAndSelect should filter data and select specified columns") {
    val reconciliationUtility = new ReconciliationUtility()

    val schema = StructType(List(
      StructField("id", StringType, nullable = true),
      StructField("name", StringType, nullable = true),
      StructField("date", StringType, nullable = true)
    ))
    val data = Seq(
      Row("1", "John Doe", "2022-01-01"),
      Row("2", "Jane Doe", "2022-01-02")
    )
    val rdd = spark.sparkContext.parallelize(data)
    val df = spark.createDataFrame(rdd, schema)

    val resultDf = reconciliationUtility.filterAndSelect(df, Array("id", "name"), "2022-01-01", "date")

    // Verify the DataFrame contains the filtered data
    // Note: In real testing, you'd assert on the content of the DataFrame.
    assert(!resultDf.rdd.isEmpty())
  }

  // Mock examples for writeToSqlServer and findAndWriteBreaks would typically involve mocking the DataFrame operations
  // and verifying that certain methods are called, as direct database interactions should be tested via integration tests.

  test("writeToSqlServer should prepare and execute write operation") {
    val mockDataFrame = mock[DataFrame]
    val reconciliationUtility = new ReconciliationUtility()

    doNothing().when(mockDataFrame).write

    // Execute
    reconciliationUtility.writeToSqlServer(mockDataFrame, Map(
      "url" -> "jdbc:sqlserver://server;databaseName=db;",
      "dbtable" -> "testTable",
      "user" -> "user",
      "password" -> "password",
      "driver" -> "com.microsoft.sqlserver.jdbc.SQLServerDriver"
    ))

    // Verify - this is symbolic; in real tests, you'd check if the write operation is configured properly.
    assert(true)
  }

  // Example test for findAndWriteBreaks would be more involved and is recommended for integration testing
  // due to its reliance on actual Spark SQL execution and database interactions.

  test("runReconciliation integrates multiple steps correctly") {
    val mockDataFrame = mock[DataFrame]
    val reconciliationUtility = new ReconciliationUtility()

    when(mockDataFrame.select(any(classOf[String]), any(classOf[String]))).thenReturn(mockDataFrame)
    when(mockDataFrame.filter(any(classOf[String]))).thenReturn(mockDataFrame)
    doNothing().when(mockDataFrame).write

    // Assuming the rest of the operations are correctly mocked/setup, this test would ensure
    // the method integrates the steps. For real testing
