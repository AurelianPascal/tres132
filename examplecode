from pyspark.sql import SparkSession
import time

# Set up Cosmos DB connection configuration
cosmosdb_config = {
  "Endpoint": "<Cosmos DB Endpoint>",
  "Masterkey": "<Cosmos DB Master Key>",
  "Database": "<Database Name>",
  "Collection": "<Collection Name>",
  "ReadChangeFeed": "true"  # Enable change feed to track progress
}

# Set up MongoDB connection configuration
mongodb_config = {
  "uri": "mongodb://<MongoDB Connection URI>",
  "database": "<Database Name>",
  "collection": "<Collection Name>"
}

# Set batch size
batch_size = 1000

# Set maximum number of retries
max_retries = 3

# Create a Spark session
spark = SparkSession.builder.appName("CosmosDB to MongoDB Migration").getOrCreate()

# Read data from Cosmos DB
try:
    cosmosdb_df = spark.read.format("cosmos.oltp").options(**cosmosdb_config).load()

    # Determine the number of batches based on the total number of documents
    num_batches = cosmosdb_df.count() // batch_size

    # Process and write data in batches with retry logic
    for i in range(num_batches + 1):
        # Select a batch of data
        start = i * batch_size
        end = (i + 1) * batch_size
        batch_df = cosmosdb_df.limit(end).filter(f"_ts > {last_processed_timestamp}")  # Use change feed to track progress if available

        # Perform data transformations if needed
        transformed_df = batch_df.select("column1", "column2", ...)  # Select required columns

        # Retry logic for writing the batch to MongoDB
        retry_count = 0
        while retry_count < max_retries:
            try:
                transformed_df.write.format("mongo").options(**mongodb_config).mode("append").save()
                break  # If successful, break out of the retry loop
            except Exception as e:
                # Handle the error, log or perform any necessary actions
                print(f"Error occurred while writing batch {i}: {str(e)}")
                retry_count += 1
                time.sleep(5)  # Wait for a few seconds before retrying

        if retry_count == max_retries:
            # If max retries reached and the batch still fails, handle accordingly
            print(f"Max retries reached for batch {i}. Skipping the batch.")

except Exception as e:
    # Handle the error, log or perform any necessary actions
    print(f"Error occurred while reading data from Cosmos DB: {str(e)}")

# Stop the Spark session
spark.stop()
