// Import necessary libraries
import org.apache.spark.sql.functions._

// Define the path to your Delta Lake table and other constants
val deltaPath = "dbfs:/path/to/your/delta/table"
val lastReconciliationTime = "2023-01-01T00:00:00"

// Dummy SQL Server connection details
val serverName = "your_server.database.windows.net"
val databaseName = "your_database_name"
val user = "your_username"
val password = "your_password"
val dbTable = "events"
val discrepancyTable = "reconciliation_discrepancies"

val readOptions = Map(
  "url" -> s"jdbc:sqlserver://$serverName;databaseName=$databaseName",
  "dbtable" -> dbTable,
  "user" -> user,
  "password" -> password,
  "partitionColumn" -> "id", // Adjust as needed
  "lowerBound" -> "1",
  "upperBound" -> "10000",
  "numPartitions" -> "4"
)

val writeOptions = Map(
  "url" -> s"jdbc:sqlserver://$serverName;databaseName=$databaseName",
  "dbtable" -> discrepancyTable,
  "user" -> user,
  "password" -> password
)

// Function to load data from Delta Lake
def loadDeltaData(deltaPath: String, lastReconciliationTime: String): DataFrame = {
  spark.read.format("delta").load(deltaPath)
    .select("id", "event_version", "last_updated_time")
    .filter(s"last_updated_time >= '$lastReconciliationTime'")
}

// Function to load data from SQL Server
def loadSqlData(readOptions: Map[String, String], lastReconciliationTime: String): DataFrame = {
  spark.read.format("com.microsoft.sqlserver.jdbc.spark")
    .options(readOptions)
    .load()
    .select("id", "event_version", "last_updated_time")
    .filter(s"last_updated_time >= '$lastReconciliationTime'")
}

// Function to write data to SQL Server
def writeDataToSql(df: DataFrame, writeOptions: Map[String, String]): Unit = {
  df.write.format("com.microsoft.sqlserver.jdbc.spark").options(writeOptions).mode("append").save()
}

// Function to identify and write discrepancies
def identifyAndWriteDiscrepancies(deltaDf: DataFrame, sqlDf: DataFrame, writeOptions: Map[String, String]): Unit = {
  val diffsToSave = deltaDf.join(sqlDf, Seq("id", "event_version"), "full_outer")
    .where(deltaDf("id").isNull || sqlDf("id").isNull)
    .select(deltaDf("id"), deltaDf("event_version"), lit("example_datatype").as("datatype"))
    .withColumn("created_at", current_timestamp())

  writeDataToSql(diffsToSave, writeOptions)
}

// Load data from Delta Lake and SQL Server
val deltaDf = loadDeltaData(deltaPath, lastReconciliationTime)
val sqlDf = loadSqlData(readOptions, lastReconciliationTime)

// Identify discrepancies and write them to SQL Server
identifyAndWriteDiscrepancies(deltaDf, sqlDf, writeOptions)

println("Differences saved successfully to SQL Server.")











////////

import org.apache.spark.sql.functions._
import org.apache.spark.sql.DataFrame

// Function to load data from Delta Lake
def loadDeltaData(deltaPath: String, lastReconciliationTime: String): DataFrame = {
  println("Loading data from Delta Lake...")
  try {
    val df = spark.read.format("delta").load(deltaPath)
      .select("id", "event_version", "last_updated_time")
      .filter(s"last_updated_time >= '$lastReconciliationTime'")
    println("Data successfully loaded from Delta Lake.")
    df
  } catch {
    case e: Exception =>
      println(s"Error loading data from Delta Lake: ${e.getMessage}")
      spark.emptyDataFrame
  }
}

// Function to load data from SQL Server
def loadSqlData(readOptions: Map[String, String], lastReconciliationTime: String): DataFrame = {
  println("Loading data from SQL Server...")
  try {
    val df = spark.read.format("com.microsoft.sqlserver.jdbc.spark")
      .options(readOptions)
      .load()
      .select("id", "event_version", "last_updated_time")
      .filter(s"last_updated_time >= '$lastReconciliationTime'")
    println("Data successfully loaded from SQL Server.")
    df
  } catch {
    case e: Exception =>
      println(s"Error loading data from SQL Server: ${e.getMessage}")
      spark.emptyDataFrame
  }
}

// Function to write data to SQL Server
def writeDataToSql(df: DataFrame, writeOptions: Map[String, String]): Unit = {
  println("Writing data to SQL Server...")
  try {
    df.write.format("com.microsoft.sqlserver.jdbc.spark").options(writeOptions).mode("append").save()
    println("Data successfully written to SQL Server.")
  } catch {
    case e: Exception =>
      println(s"Error writing data to SQL Server: ${e.getMessage}")
  }
}

// Function to identify and write discrepancies
def identifyAndWriteDiscrepancies(deltaDf: DataFrame, sqlDf: DataFrame, writeOptions: Map[String, String]): Unit = {
  println("Identifying discrepancies...")
  if (deltaDf.isEmpty || sqlDf.isEmpty) {
    println("One of the DataFrames is empty, skipping discrepancies identification.")
    return
  }
  try {
    val diffsToSave = deltaDf.join(sqlDf, Seq("id", "event_version"), "full_outer")
      .where(deltaDf("id").isNull || sqlDf("id").isNull)
      .select(deltaDf("id"), deltaDf("event_version"), lit("example_datatype").as("datatype"))
      .withColumn("created_at", current_timestamp())

    if (diffsToSave.isEmpty) {
      println("No discrepancies found.")
    } else {
      writeDataToSql(diffsToSave, writeOptions)
      println("Discrepancies successfully identified and saved.")
    }
  } catch {
    case e: Exception =>
      println(s"Error identifying or writing discrepancies: ${e.getMessage}")
  }
}

// Define your configurations and paths
val deltaPath = "dbfs:/path/to/your/delta/table"
val lastReconciliationTime = "2023-01-01T00:00:00"
val readOptions = Map(/* Your SQL Server read options here */)
val writeOptions = Map(/* Your SQL Server write options here */)

// Execution
try {
  val deltaDf = loadDeltaData(deltaPath, lastReconciliationTime)
  val sqlDf = loadSqlData(readOptions, lastReconciliationTime)
  identifyAndWriteDiscrepancies(deltaDf, sqlDf, writeOptions)
} catch {
  case e: Exception => println(s"Unexpected error: ${e.getMessage}")
}

